{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1, 80, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_u = [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_ch = 80\n",
    "new_ch = 512\n",
    "stride = 128\n",
    "conv_init = nn.Conv1d(prev_ch, new_ch, kernel_size=3, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_ch = new_ch\n",
    "new_ch = int(new_ch/2)\n",
    "stride = int(stride/2)\n",
    "first = nn.ConvTranspose1d(prev_ch, new_ch, kernel_size=3, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvTranspose1d(3, 1, kernel_size=(7, 1), stride=(1,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ConvTranspose1d(in_channels=3, out_channels=1, kernel_size=[7,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 4, 8] n=32 x∈[-1.185, 1.046] μ=-0.030 σ=0.531 grad ConvolutionBackward0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = nn.Conv1d(3, 4, 3)\n",
    "a = torch.randn(1, 3,  10)\n",
    "test(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlk(nn.Module):\n",
    "    def __init__(self, kernel, dilations, channel):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for l in range(len(dilations)):\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            layers.append(nn.Conv1d(in_channels=channel, out_channels=channel,\n",
    "                                    kernel_size=kernel, dilation=dilations[l]))\n",
    "\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRF(nn.Module):\n",
    "    def __init__(self, k_r, D_r, channel):\n",
    "        super().__init__()\n",
    "        self.k_r = k_r\n",
    "        self.D_r = D_r\n",
    "        layers = []\n",
    "        for n in range(len(D_r)):\n",
    "            layers.append(ResBlk(k_r[n], D_r[n], channel))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h=layer(h)+h\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(1,3,32)\n",
    "l1 = nn.Conv1d(3, 10, kernel_size=3,stride=1,padding=1, dilation=2)\n",
    "l2 = nn.Conv1d(10, 20, kernel_size=3, stride=1, padding=1, dilation=4)\n",
    "\n",
    "skip = nn.Conv1d(3, 20, kernel_size=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 20, 24] n=480 x∈[-1.153, 1.173] μ=-0.014 σ=0.377 grad ConvolutionBackward0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2(l1(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(kernel, dilation):\n",
    "    return int((kernel*dilation - dilation)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding을 적절히 조절해서 shape을 맞추도록 함\n",
    "l1 = nn.Conv1d(3, 3, kernel_size=3, stride=1, dilation=1, padding=get_padding(3, 1))\n",
    "l2 = nn.Conv1d(3, 3, kernel_size=3, stride=1, dilation=1, padding=get_padding(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 3, 32] n=96 x∈[-1.083, 0.763] μ=-0.390 σ=0.376 grad ConvolutionBackward0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(1,3,32)\n",
    "l2(l1(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlk(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, dilation=(1,3,5)):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.ModuleList(nn.Conv1d(),\n",
    "                                   nn.Conv1d(),\n",
    "                                   nn.Conv1d())\n",
    "        self.conv2 = nn.ModuleList(nn.Conv1d(),\n",
    "                                   nn.Conv1d(),\n",
    "                                   nn.Conv1d())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c1, c2 in zip(self.conv1, self.conv2):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_u = [16,16,4,4]\n",
    "k_r = [3,7,11]\n",
    "h_u = 512\n",
    "layers = []\n",
    "D_r = [[1, 1], [3, 1], [5, 1]]\n",
    "channel = 32\n",
    "test = MRF(k_r, D_r, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MRF(\n",
       "  (layers): Sequential(\n",
       "    (0): ResBlk(\n",
       "      (layers): Sequential(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): ResBlk(\n",
       "      (layers): Sequential(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(3,))\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlk(\n",
       "      (layers): Sequential(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), dilation=(5,))\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Conv1d(32, 32, kernel_size=(11,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (96) must match the size of tensor b (100) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m1\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m100\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mMRF.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m h \u001b[39m=\u001b[39m x\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 15\u001b[0m     h\u001b[39m=\u001b[39mlayer(h)\u001b[39m+\u001b[39;49mh\n\u001b[1;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m h\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (96) must match the size of tensor b (100) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "test(torch.randn(1, 32, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_u = [16,16,4,4]\n",
    "k_r = [3,7,11]\n",
    "h_u = 512\n",
    "layers = []\n",
    "D_r = [[1, 1], [3, 1], [5, 1]]\n",
    "\n",
    "layers.append(nn.Conv1d(in_channels=1, out_channels=h_u, kernel_size=[7,1], dilation=1))\n",
    "prev_ch = h_u\n",
    "for l in range(len(k_u)):\n",
    "    new_ch = prev_ch //2\n",
    "    layers.append(nn.LeakyReLU())\n",
    "    stride = k_u[l]/2\n",
    "    layers.append(nn.ConvTranspose1d(prev_ch, new_ch, kernel_size=3, stride=stride))\n",
    "    layers.append(MRF(k_r, D_r, new_ch))\n",
    "    prev_ch = new_ch \n",
    "layers.append(nn.LeakyReLU())\n",
    "layers.append(nn.Conv1d(in_channels=prev_ch, out_channels=1, kernel_size=[7,1]))\n",
    "layers.append(nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(1, 512, kernel_size=(7, 1), stride=(1,))\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): ConvTranspose1d(512, 256, kernel_size=(3,), stride=(8.0,))\n",
       "  (3): MRF(\n",
       "    (layers): Sequential(\n",
       "      (0): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), dilation=(3,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(256, 256, kernel_size=(7,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), dilation=(5,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(256, 256, kernel_size=(11,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): LeakyReLU(negative_slope=0.01)\n",
       "  (5): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(8.0,))\n",
       "  (6): MRF(\n",
       "    (layers): Sequential(\n",
       "      (0): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), dilation=(3,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(128, 128, kernel_size=(7,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), dilation=(5,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(128, 128, kernel_size=(11,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (7): LeakyReLU(negative_slope=0.01)\n",
       "  (8): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(2.0,))\n",
       "  (9): MRF(\n",
       "    (layers): Sequential(\n",
       "      (0): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), dilation=(3,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), dilation=(5,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(64, 64, kernel_size=(11,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (10): LeakyReLU(negative_slope=0.01)\n",
       "  (11): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2.0,))\n",
       "  (12): MRF(\n",
       "    (layers): Sequential(\n",
       "      (0): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), dilation=(3,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlk(\n",
       "        (layers): Sequential(\n",
       "          (0): LeakyReLU(negative_slope=0.01)\n",
       "          (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), dilation=(5,))\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "          (3): Conv1d(32, 32, kernel_size=(11,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (13): LeakyReLU(negative_slope=0.01)\n",
       "  (14): Conv1d(32, 1, kernel_size=(7, 1), stride=(1,))\n",
       "  (15): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def index(feat, uv):\n",
    "    '''\n",
    "    extract image features at floating coordinates with bilinear interpolation\n",
    "    args:\n",
    "        feat: [B, C, H, W] image features\n",
    "        uv: [B, 2, N] normalized image coordinates ranged in [-1, 1]\n",
    "    return:\n",
    "        [B, C, N] sampled pixel values\n",
    "    '''\n",
    "    uv = uv.transpose(1, 2)\n",
    "    uv = uv.unsqueeze(2)\n",
    "    samples = torch.nn.functional.grid_sample(feat, uv, align_corners=True)\n",
    "    return samples[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 1, 5, 5] i64 n=25 x∈[0, 24] μ=12.000 σ=7.360\n",
       "tensor([[[[ 0,  1,  2,  3,  4],\n",
       "          [ 5,  6,  7,  8,  9],\n",
       "          [10, 11, 12, 13, 14],\n",
       "          [15, 16, 17, 18, 19],\n",
       "          [20, 21, 22, 23, 24]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define an input tensor with size (1, 1, 3, 3)\n",
    "input_tensor = torch.arange(25).reshape((1,1,5,5))\n",
    "input_tensor.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a transformation grid with size (1, 2, 3)\n",
    "grid = torch.tensor([[[0.5, 0.5, 0], [0.5, 0.5, 0]]])\n",
    "\n",
    "# Apply the grid_sample function\n",
    "output = F.grid_sample(input_tensor, grid)\n",
    "\n",
    "# Print the output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
